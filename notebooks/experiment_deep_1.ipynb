{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment CNN models\n",
    "Use this to test different CNN models, augmentations, and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_cv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random variables\n",
    "np.random.seed(0)\n",
    "\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our data\n",
    "\n",
    "TRAIN_DATADIR = \"../Dataset/train_directory\"\n",
    "VAL_DATADIR = \"../Dataset/val_directory\"\n",
    "TEST_DATADIR = \"../Dataset/test_directory\"\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_DATADIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(224, 224),\n",
    "    shuffle=True,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.image_dataset_from_directory(\n",
    "    VAL_DATADIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(224, 224),\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.image_dataset_from_directory(\n",
    "    TEST_DATADIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=1,\n",
    "    image_size=(224, 224),\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featch url and labels to a datafram for\n",
    "\n",
    "data = []\n",
    "\n",
    "# Traverse each class directory\n",
    "for class_dir in os.listdir(TRAIN_DATADIR):\n",
    "    for img in os.listdir(os.path.join(TRAIN_DATADIR, class_dir)):\n",
    "        data.append((os.path.join(TRAIN_DATADIR, class_dir, img), class_dir))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['filepath', 'label'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the class weights\n",
    "class_labels = df['label'].unique()\n",
    "label_to_index = {label: idx for idx, label in enumerate(df['label'].unique())}\n",
    "weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=df['label'])\n",
    "\n",
    "class_weights = dict(zip(label_to_index.values(), weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the data\n",
    "batch = next(iter(train_ds.take(1)))\n",
    "image_batch = batch[0]\n",
    "\n",
    "keras_cv.visualization.plot_image_gallery(\n",
    "    image_batch,\n",
    "    rows=3,\n",
    "    cols=3,\n",
    "    value_range=(0, 255),\n",
    "    show=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentations\n",
    "- RandomFlip (horizontal and vertical)\n",
    "- RandomCropAndResize\n",
    "- Normalize pixel values to [0, 1]\n",
    "\n",
    "The other augmentations are more experimental and can be skipped or added to test their effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly flip the image horizontally and vertically\n",
    "import keras_cv.layers.preprocessing\n",
    "\n",
    "\n",
    "random_flip = keras_cv.layers.RandomFlip(mode=\"horizontal_and_vertical\")\n",
    "\n",
    "# Randomly crop and resize the image\n",
    "crop_and_resize = keras_cv.layers.RandomCropAndResize(\n",
    "    target_size=(224, 224),\n",
    "    crop_area_factor=(0.8, 1.0),\n",
    "    aspect_ratio_factor=(0.9, 1.1)\n",
    ")\n",
    "\n",
    "# Apply some random augmentations\n",
    "rand_augment = keras_cv.layers.RandAugment(\n",
    "    augmentations_per_image=3,\n",
    "    value_range=(0, 1),\n",
    "    magnitude=0.5,\n",
    "    magnitude_stddev=0.2,\n",
    "    rate=1.0\n",
    ")\n",
    "\n",
    "# Cut parts of the image and paste them on other images\n",
    "cut_mix = keras_cv.layers.preprocessing.CutMix()\n",
    "\n",
    "# Mix two images together\n",
    "mix_up = keras_cv.layers.preprocessing.MixUp()\n",
    "\n",
    "# Randomly choose between CutMix and MixUp\n",
    "cut_mix_or_mix_up = keras_cv.layers.RandomChoice([cut_mix, mix_up], batchwise=True)\n",
    "\n",
    "# Define the augmentation function\n",
    "def augmenter_train(images, labels):\n",
    "    images = tf.cast(images, tf.float32) / 255.0\n",
    "    images = random_flip(images, training=True)\n",
    "    images = crop_and_resize(images, training=True)\n",
    "    #inputs = rand_augment(inputs, training=True)\n",
    "    #inputs = cut_mix_or_mix_up(inputs)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def augmenter_val(images, labels):\n",
    "    images = tf.cast(images, tf.float32) / 255.0\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(augmenter_train, num_parallel_calls=tf.data.AUTOTUNE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(augmenter_val, num_parallel_calls=tf.data.AUTOTUNE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = test_ds.map(augmenter_val, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch = next(iter(train_ds.take(1)))[0]\n",
    "\n",
    "keras_cv.visualization.plot_image_gallery(\n",
    "    image_batch,\n",
    "    rows=3,\n",
    "    cols=3,\n",
    "    value_range=(0, 1),\n",
    "    show=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates: https://arxiv.org/abs/1708.07120 <br>\n",
    "CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features: https://arxiv.org/abs/1905.04899 <br>\n",
    "mixup: Beyond Empirical Risk Minimization: https://arxiv.org/abs/1710.09412 <br>\n",
    "https://github.com/ageron/handson-ml3/blob/main/11_training_deep_neural_networks.ipynb \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ExponentialLearningRate(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.sum_of_epoch_losses = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        mean_epoch_loss = logs[\"loss\"]\n",
    "        new_sum_of_epoch_losses = mean_epoch_loss * (batch + 1)\n",
    "        batch_loss = new_sum_of_epoch_losses - self.sum_of_epoch_losses\n",
    "        self.sum_of_epoch_losses = new_sum_of_epoch_losses\n",
    "        self.rates.append(self.model.optimizer.learning_rate.numpy())\n",
    "        self.losses.append(batch_loss)\n",
    "        new_lr = self.model.optimizer.learning_rate * self.factor\n",
    "        self.model.optimizer.learning_rate.assign(new_lr)\n",
    "        \n",
    "def find_learning_rate(model, dataset, epochs=1, min_rate=1e-4, max_rate=1):\n",
    "    init_weights = model.get_weights()\n",
    "    num_samples = tf.data.experimental.cardinality(dataset).numpy()\n",
    "    iterations = math.ceil(num_samples / epochs)\n",
    "    factor = (max_rate / min_rate) ** (1 / iterations)\n",
    "    init_lr = model.optimizer.learning_rate.numpy()\n",
    "    model.optimizer.learning_rate.assign(min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(dataset, epochs=epochs, callbacks=[exp_lr])\n",
    "    model.optimizer.learning_rate.assign(init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses, \"b\")\n",
    "    plt.gca().set_xscale('log')\n",
    "    max_loss = losses[0] + min(losses)\n",
    "    plt.hlines(min(losses), min(rates), max(rates), color=\"k\")\n",
    "    plt.axis([min(rates), max(rates), 0, max_loss])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_lr=1e-3, start_lr=None,\n",
    "                 last_iterations=None, last_lr=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_lr = max_lr\n",
    "        self.start_lr = start_lr or max_lr / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_lr = last_lr or self.start_lr / 1000\n",
    "        self.iteration = 0\n",
    "\n",
    "    def _interpolate(self, iter1, iter2, lr1, lr2):\n",
    "        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
    "\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            lr = self._interpolate(0, self.half_iteration, self.start_lr,\n",
    "                                   self.max_lr)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            lr = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                   self.max_lr, self.start_lr)\n",
    "        else:\n",
    "            lr = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                   self.start_lr, self.last_lr)\n",
    "        self.iteration += 1\n",
    "        self.model.optimizer.learning_rate.assign(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-34 CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1,\n",
    "                        padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                        use_bias=False)\n",
    "\n",
    "\n",
    "@register_keras_serializable(package='Custom', name='ResidualUnit')\n",
    "class ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.strides = strides\n",
    "        self.filters = filters\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ]\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "                \"filters\": self.filters,\n",
    "                \"strides\": self.strides,\n",
    "                \"activation\": tf.keras.activations.serialize(self.activation)\n",
    "            })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(224, 224, 3)),\n",
    "    DefaultConv2D(64, kernel_size=7, strides=2),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"),\n",
    "])\n",
    "prev_filters = 64\n",
    "for filters in [64] * 2 + [128] * 2 + [256] * 2 + [512] * 2:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "\n",
    "model.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "model.add(tf.keras.layers.Dense(11, activation=\"softmax\"))\n",
    "\n",
    "loss = keras.losses.CategoricalFocalCrossentropy(weights)\n",
    "\n",
    "model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum = 0.9, weight_decay=5e-4),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal learning rate\n",
    "rates, losses = find_learning_rate(model, train_ds, epochs=1)\n",
    "plot_lr_vs_loss(rates, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.CategoricalFocalCrossentropy(weights),\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum = 0.9, weight_decay=5e-4),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "EPOCHS = 75\n",
    "# Set up callbacks\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "import datetime\n",
    "\n",
    "log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "onecycle = OneCycleScheduler(math.ceil(len(train_ds)) * EPOCHS, max_lr=1e-3)\n",
    "\n",
    "\n",
    "# Callbacks definition\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=20, verbose=1),\n",
    "    ModelCheckpoint('../models/best_model_18.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
    "    onecycle\n",
    "]\n",
    "\n",
    "\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curves(history, start_epoch=1):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy curves.\n",
    "\n",
    "    Parameters:\n",
    "    - history: Training history (output from the model's fit method).\n",
    "    - start_epoch: Epoch from which to start plotting. Default is 5 (i.e., plot from epoch 6 onwards).\n",
    "    \"\"\"   \n",
    "\n",
    "# Convert the history.history dict to a pandas DataFrame\n",
    "    df = pd.DataFrame(history.history)\n",
    "\n",
    "    # Plot the curves from the specified epoch onwards\n",
    "    df = df.iloc[start_epoch-1:]\n",
    "\n",
    "    # Set the style of seaborn for better visualization\n",
    "    sns.set(rc={'axes.facecolor': '#f0f0fc'}, style='darkgrid')\n",
    "\n",
    "    # Plotting the learning curves\n",
    "    plt.figure(figsize=(15,6))\n",
    "\n",
    "    # Plotting the training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.lineplot(x=df.index, y=df['loss'], color='royalblue', label='Train Loss')\n",
    "    sns.lineplot(x=df.index, y=df['val_loss'], color='orangered', linestyle='--', label='Validation Loss')\n",
    "    plt.title('Loss Evolution')\n",
    "\n",
    "    # Plotting the training and validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.lineplot(x=df.index, y=df['accuracy'], color='royalblue', label='Train Accuracy')\n",
    "    sns.lineplot(x=df.index, y=df['val_accuracy'], color='orangered', linestyle='--', label='Validation Accuracy')\n",
    "    plt.title('Accuracy Evolution')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(history, start_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "def evaluate_model_performance(model, test_ds, class_labels):\n",
    "    \"\"\"\n",
    "    Evaluate the model's performance on the validation set and print the classification report.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model.\n",
    "    - val_generator: Validation data generator.\n",
    "    - class_labels: List of class names.\n",
    "    \n",
    "    Returns:\n",
    "    - report: Classification report as a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a list to hold all labels\n",
    "    true_labels = []\n",
    "\n",
    "    # Iterate over the dataset\n",
    "    for images, labels in test_ds:\n",
    "        true_labels.append(np.argmax(labels.numpy()))  # Extract labels and convert to NumPy arrays\n",
    " \n",
    "    #  To get the predicted labels, we predict using the model  \n",
    "    predictions = model.predict(test_ds, steps=len(test_ds))\n",
    "    \n",
    "    # Take the argmax to get the predicted class indices.\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    print(predicted_labels)\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(true_labels, predicted_labels, target_names=class_labels)\n",
    "    print(report)\n",
    "    print('\\n')\n",
    "    \n",
    "    # Define a custom colormap\n",
    "    colors = [\"white\", \"royalblue\"]\n",
    "    cmap_cm = LinearSegmentedColormap.from_list(\"cmap_cm\", colors)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Plotting confusion matrix using seaborn\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, cmap=cmap_cm, fmt='d', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impoer model\n",
    "model = keras.models.load_model('../models/best_model_18.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['battery', 'biological', 'brown-glass', 'cardboard', 'green-glass', 'metal', 'paper', 'plastic', 'textile','vegetation', 'white-glass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(model, test_ds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "\n",
    "def test_model(model, test_ds, labels):\n",
    "    test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "    print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Get the true labels and predicted labels\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for image_batch, label_batch in test_ds:\n",
    "        y_true.extend(label_batch.numpy())\n",
    "        y_pred.extend(model.predict(image_batch).argmax(axis=1))\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Display the confusion matrix using ConfusionMatrixDisplay\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap='Blues', xticks_rotation='vertical')\n",
    "    plt.show()\n",
    "\n",
    "    # Display the classification report\n",
    "    print(classification_report(y_true, y_pred, target_names=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, test_ds, df['label'].unique())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tumor-segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
