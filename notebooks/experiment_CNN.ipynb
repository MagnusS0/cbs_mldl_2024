{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import keras_cv\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our data\n",
    "\n",
    "TRAIN_DATADIR = \"../data/train_directory\"\n",
    "VAL_DATADIR = \"../data/val_directory\"\n",
    "TEST_DATADIR = \"../data/test_directory\"\n",
    "BATCH_SIZE = 128  # Change if running into memory issues\n",
    "\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_DATADIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(224, 224),\n",
    "    shuffle=True,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.image_dataset_from_directory(\n",
    "    VAL_DATADIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(224, 224),\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.image_dataset_from_directory(\n",
    "    TEST_DATADIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=1,\n",
    "    image_size=(224, 224),\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly flip the image horizontally and vertically\n",
    "random_flip = keras_cv.layers.RandomFlip(mode=\"horizontal_and_vertical\")\n",
    "\n",
    "# Randomly crop and resize the image\n",
    "crop_and_resize = keras_cv.layers.RandomCropAndResize(\n",
    "    target_size=(224, 224),\n",
    "    crop_area_factor=(0.8, 1.0),\n",
    "    aspect_ratio_factor=(0.9, 1.1)\n",
    ")\n",
    "\n",
    "# Apply some random augmentations\n",
    "rand_augment = keras_cv.layers.RandAugment(\n",
    "    augmentations_per_image=3,\n",
    "    value_range=(0, 1),\n",
    "    magnitude=0.5,\n",
    "    magnitude_stddev=0.2,\n",
    "    rate=1.0\n",
    ")\n",
    "\n",
    "# Merge multiple augmentations into a single augmentation\n",
    "# Stays more true to the original image than cutmix or mixup\n",
    "aug_mix = keras_cv.layers.AugMix(\n",
    "    [0,1],\n",
    "    severity=0.3,\n",
    "    num_chains=3,\n",
    "    chain_depth=[1, 3],\n",
    "    alpha=1.0,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "# Cut parts of the image and paste them on other images\n",
    "cut_mix = keras_cv.layers.preprocessing.CutMix()\n",
    "\n",
    "# Mix two images together\n",
    "mix_up = keras_cv.layers.preprocessing.MixUp()\n",
    "\n",
    "# Randomly choose between CutMix and MixUp\n",
    "cut_mix_or_mix_up = keras_cv.layers.RandomChoice([cut_mix, mix_up], batchwise=True)\n",
    "\n",
    "# Define the augmentation function\n",
    "def augmenter_train(images, labels):\n",
    "    images = tf.cast(images, tf.float32) / 255.0\n",
    "    images = random_flip(images, training=True)\n",
    "    images = crop_and_resize(images, training=True)\n",
    "    #inputs = rand_augment(inputs, training=True)\n",
    "    #images = aug_mix(images, training=True)\n",
    "    #inputs = cut_mix_or_mix_up(inputs)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def augmenter_val(images, labels):\n",
    "    images = tf.cast(images, tf.float32) / 255.0\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(augmenter_train, num_parallel_calls=tf.data.AUTOTUNE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(augmenter_val, num_parallel_calls=tf.data.AUTOTUNE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = test_ds.map(augmenter_val, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    'best_model.keras',                 # Path where the model is saved\n",
    "    monitor='val_loss',                 # Metric to monitor\n",
    "    save_best_only=True,                # Save only the best model\n",
    "    save_weights_only=False,            # Save entire model not just weights\n",
    "    verbose=1                           # Logging level\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',                 # Metric to monitor\n",
    "    patience=10,                        # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1                           # Logging level\n",
    ")\n",
    "\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss',                 # Metric to monitor\n",
    "    factor=0.2,                         # Factor by which the learning rate will be reduced\n",
    "    patience=5,                         # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=0.001,                       # Lower bound on the learning rate\n",
    "    verbose=1                           # Logging level\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Architecture of Model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "\n",
    "def create_model(filters=(64, 128, 256), kernel_size=(7, 7),\n",
    "                 add_conv_block=False, extra_dense_layers=0, use_pooling=True, img_height=IMG_HEIGHT, img_width=IMG_WIDTH):\n",
    "    model = Sequential()\n",
    "\n",
    "    # First Convolutional Block\n",
    "    model.add(Conv2D(filters[0], kernel_size, activation='relu', padding='same', input_shape=(img_height, img_width, 3), kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    if use_pooling:\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    model.add(Conv2D(filters[1], kernel_size, activation='relu', padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    if use_pooling:\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Optional Third Convolutional Block\n",
    "    model.add(Conv2D(filters[2], kernel_size, activation='relu', padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    if add_conv_block:\n",
    "        model.add(Conv2D(filters[2], kernel_size, activation='relu', padding='same', kernel_initializer='he_normal'))\n",
    "        model.add(BatchNormalization())\n",
    "        if use_pooling:\n",
    "            model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Global Average Pooling\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    # Base Dense Layers\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Additional Dense Layers based on the parameter\n",
    "    for _ in range(extra_dense_layers):\n",
    "        model.add(Dense(256, activation='relu', kernel_initializer='he_normal'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models_to_test, train_dataset, val_dataset):\n",
    "    results = []\n",
    "    for model in models_to_test:\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            steps_per_epoch=len(train_dataset)//BATCH_SIZE,\n",
    "            epochs=10,\n",
    "            validation_data=val_dataset,\n",
    "            validation_steps=len(val_dataset)//BATCH_SIZE,\n",
    "            callbacks=[checkpoint_callback, early_stopping_callback, reduce_lr_callback]\n",
    "        )\n",
    "        max_val_accuracy = max(history.history['val_accuracy'])\n",
    "        results.append((model, max_val_accuracy))\n",
    "    return results\n",
    "\n",
    "\n",
    "# Define, create, evaluate models with various configurations\n",
    "configs = [\n",
    "    {'filters': (64, 128, 256), 'kernel_size': (7, 7), 'add_conv_block': False, 'extra_dense_layers': 0, 'use_pooling': True},\n",
    "    {'filters': (64, 128, 256), 'kernel_size': (7, 7), 'add_conv_block': True, 'extra_dense_layers': 0, 'use_pooling': True},\n",
    "    {'filters': (64, 128, 256), 'kernel_size': (7, 7), 'add_conv_block': False, 'extra_dense_layers': 1, 'use_pooling': True},\n",
    "    {'filters': (64, 128, 256), 'kernel_size': (7, 7), 'add_conv_block': True, 'extra_dense_layers': 1, 'use_pooling': True},\n",
    "    {'filters': (64, 128, 256), 'kernel_size': (7, 7), 'add_conv_block': False, 'extra_dense_layers': 3, 'use_pooling': True},\n",
    "    {'filters': (64, 128, 256), 'kernel_size': (7, 7), 'add_conv_block': True, 'extra_dense_layers': 3, 'use_pooling': True},\n",
    "    {'filters': (64, 128, 256), 'kernel_size': (7, 7), 'add_conv_block': False, 'extra_dense_layers': 2, 'use_pooling': True},\n",
    "    {'filters': (64, 128, 256), 'kernel_size': (7, 7), 'add_conv_block': True, 'extra_dense_layers': 2, 'use_pooling': True},\n",
    "    {'filters': (32, 64, 128), 'kernel_size': (7, 7), 'add_conv_block': False, 'extra_dense_layers': 0, 'use_pooling': True},\n",
    "]\n",
    "\n",
    "models_to_test = [create_model(**config) for config in configs]\n",
    "results = evaluate_models(models_to_test, train_ds, val_ds)\n",
    "\n",
    "# Print out results\n",
    "for config, (model, accuracy) in zip(configs, results):\n",
    "    print(\"Model configuration:\", config)\n",
    "    print(\"Achieved maximum validation accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "*   Model configuration: {'filters': (64, 128, 256), 'kernel_size': (7, 7), 'add_conv_block': False, 'extra_dense_layers': 0, 'use_pooling': True}\n",
    "Achieved maximum validation accuracy: 0.6000000238418579\n",
    "*Model configuration: {'filters': (64, 128, 256), 'kernel_size': (7, 7), 'add_conv_block': True, 'extra_dense_layers': 0, 'use_pooling': True}\n",
    "Achieved maximum validation accuracy: 0.6000000238418579\n",
    "* Model configuration: {'filters': (64, 128, 256), 'kernel_size': (7, 7), 'add_conv_block': False, 'extra_dense_layers': 1, 'use_pooling': True}\n",
    "Achieved maximum validation accuracy: 0.4000000059604645\n",
    "* Model configuration: {'filters': (64, 128, 256), 'kernel_size': (7, 7), 'add_conv_block': True, 'extra_dense_layers': 1, 'use_pooling': True}\n",
    "Achieved maximum validation accuracy: 0.3456125855445862\n",
    "* Model configuration: {'filters': (64, 128, 256), 'kernel_size': (7, 7), 'add_conv_block': False, 'extra_dense_layers': 3, 'use_pooling': True}\n",
    "Achieved maximum validation accuracy: 0.2574503421783447\n",
    "* Model configuration: {'filters': (64, 128, 256), 'kernel_size': (7, 7), 'add_conv_block': True, 'extra_dense_layers': 3, 'use_pooling': True}\n",
    "Achieved maximum validation accuracy: 0.4000000059604645\n",
    "* Model configuration: {'filters': (64, 128, 256), 'kernel_size': (7, 7), 'add_conv_block': False, 'extra_dense_layers': 2, 'use_pooling': True}\n",
    "Achieved maximum validation accuracy: 0.4000000059604645\n",
    "* Model configuration: {'filters': (64, 128, 256), 'kernel_size': (7, 7), 'add_conv_block': True, 'extra_dense_layers': 2, 'use_pooling': True}\n",
    "Achieved maximum validation accuracy: 0.4000000059604645\n",
    "* Model configuration: {'filters': (32, 64, 128), 'kernel_size': (7, 7), 'add_conv_block': False, 'extra_dense_layers': 0, 'use_pooling': True}\n",
    "Achieved maximum validation accuracy: 0.6000000238418579"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # First Convolutional Block\n",
    "    model.add(Conv2D(64, (7, 7), activation='relu', padding='same', input_shape=(256, 256, 3), kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    model.add(Conv2D(128, (7, 7), activation='relu', padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Third Convolutional Block\n",
    "    model.add(Conv2D(256, (7, 7), activation='relu', padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Global Average Pooling\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    # Dense Layers\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(hp.Choice('dropout_rate', values=[0.3, 0.5, 0.7])))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(hp.Choice('dropout_rate', values=[0.3, 0.5, 0.7])))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "    learning_rate = hp.Choice('learning_rate', values=[0.0001, 0.001, 0.01, 0.1])\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='logs',\n",
    "    project_name='hyperparam_tuning'\n",
    ")\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Run the hyperparameter search\n",
    "tuner.search(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f'Best Hyperparameters: {best_hps.values}')\n",
    "\n",
    "# Build the best model and train it\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training with Learning Rate: 0.0001, Batch Size: 16, Dropout Rate: 0.3**\n",
    "\n",
    "\n",
    "Trial 1:\n",
    "Hyperparameters: {'dropout_rate': 0.3, 'learning_rate': 0.0001}\n",
    "Score: 0.6480793356895447\n",
    "\n",
    "Trial 2:\n",
    "Hyperparameters: {'dropout_rate': 0.3, 'learning_rate': 0.01}\n",
    "Score: 0.5555555820465088\n",
    "\n",
    "Trial 3:\n",
    "Hyperparameters: {'dropout_rate': 0.7, 'learning_rate': 0.0001}\n",
    "Score: 0.5340768098831177\n",
    "\n",
    "Trial 4:\n",
    "Hyperparameters: {'dropout_rate': 0.5, 'learning_rate': 0.001}\n",
    "Score: 0.47211897373199463\n",
    "\n",
    "Trial 5:\n",
    "Hyperparameters: {'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
    "Score: 0.45105329155921936\n",
    "\n",
    "Trial 6:\n",
    "Hyperparameters: {'dropout_rate': 0.7, 'learning_rate': 0.01}\n",
    "Score: 0.4089219272136688\n",
    "\n",
    "Trial 7:\n",
    "Hyperparameters: {'dropout_rate': 0.5, 'learning_rate': 0.01}\n",
    "Score: 0.3238331377506256\n",
    "\n",
    "Trial 8:\n",
    "Hyperparameters: {'dropout_rate': 0.7, 'learning_rate': 0.001}\n",
    "Score: 0.29120197892189026\n",
    "\n",
    "Trial 9:\n",
    "Hyperparameters: {'dropout_rate': 0.5, 'learning_rate': 0.1}\n",
    "Score: 0.26517966389656067\n",
    "\n",
    "Trial 10:\n",
    "Hyperparameters: {'dropout_rate': 0.3, 'learning_rate': 0.1}\n",
    "Score: 0.2593969404697418\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tumor-segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
