{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment CNN models\n",
    "Use this to test different CNN models, augmentations, and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_cv\n",
    "import keras_cv.layers.preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random variables\n",
    "np.random.seed(0)\n",
    "\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our data\n",
    "\n",
    "TRAIN_DATADIR = \"../data/train_directory\"\n",
    "VAL_DATADIR = \"../data/val_directory\"\n",
    "TEST_DATADIR = \"../data/test_directory\"\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_DATADIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(224, 224),\n",
    "    shuffle=True,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.image_dataset_from_directory(\n",
    "    VAL_DATADIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(224, 224),\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.image_dataset_from_directory(\n",
    "    TEST_DATADIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=1,\n",
    "    image_size=(224, 224),\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve class names\n",
    "class_names = train_ds.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class weights\n",
    "\n",
    "def compute_class_weights(dataset_dir):\n",
    "    # Extract class weights for imbalanced datasets\n",
    "    data = []\n",
    "    for class_dir in os.listdir(dataset_dir):\n",
    "        for img in os.listdir(os.path.join(dataset_dir, class_dir)):\n",
    "            data.append((os.path.join(dataset_dir, class_dir, img), class_dir))\n",
    "    df = pd.DataFrame(data, columns=['filepath', 'label'])\n",
    "    class_labels = df['label'].unique()\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=df['label'].values)\n",
    "    return weights\n",
    "\n",
    "weights = compute_class_weights(TRAIN_DATADIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the data\n",
    "batch = next(iter(train_ds.take(1)))\n",
    "image_batch = batch[0]\n",
    "\n",
    "keras_cv.visualization.plot_image_gallery(\n",
    "    image_batch,\n",
    "    rows=3,\n",
    "    cols=3,\n",
    "    value_range=(0, 255),\n",
    "    show=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentations\n",
    "- RandomFlip (horizontal and vertical)\n",
    "- Random Crop And Resize\n",
    "- Normalize pixel values to [0, 1]\n",
    "\n",
    "The other augmentations are more experimental and can be skipped or added to test their effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly flip the image horizontally and vertically\n",
    "random_flip = keras_cv.layers.RandomFlip(mode=\"horizontal_and_vertical\")\n",
    "\n",
    "# Randomly crop and resize the image\n",
    "crop_and_resize = keras_cv.layers.RandomCropAndResize(\n",
    "    target_size=(224, 224),\n",
    "    crop_area_factor=(0.8, 1.0),\n",
    "    aspect_ratio_factor=(0.9, 1.1)\n",
    ")\n",
    "\n",
    "# Apply some random augmentations\n",
    "rand_augment = keras_cv.layers.RandAugment(\n",
    "    augmentations_per_image=3,\n",
    "    value_range=(0, 1),\n",
    "    magnitude=0.5,\n",
    "    magnitude_stddev=0.2,\n",
    "    rate=1.0\n",
    ")\n",
    "\n",
    "# Merge multiple augmentations into a single augmentation\n",
    "# Stays more true to the original image than cutmix or mixup\n",
    "aug_mix = keras_cv.layers.AugMix(\n",
    "    [0,1],\n",
    "    severity=0.3,\n",
    "    num_chains=3,\n",
    "    chain_depth=[1, 3],\n",
    "    alpha=1.0,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "# Cut parts of the image and paste them on other images\n",
    "cut_mix = keras_cv.layers.preprocessing.CutMix()\n",
    "\n",
    "# Mix two images together\n",
    "mix_up = keras_cv.layers.preprocessing.MixUp()\n",
    "\n",
    "# Randomly choose between CutMix and MixUp\n",
    "cut_mix_or_mix_up = keras_cv.layers.RandomChoice([cut_mix, mix_up], batchwise=True)\n",
    "\n",
    "# Define the augmentation function\n",
    "def augmenter_train(images, labels):\n",
    "    images = tf.cast(images, tf.float32) / 255.0\n",
    "    images = random_flip(images, training=True)\n",
    "    images = crop_and_resize(images, training=True)\n",
    "    #inputs = rand_augment(inputs, training=True)\n",
    "    #images = aug_mix(images, training=True)\n",
    "    #inputs = cut_mix_or_mix_up(inputs)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def augmenter_val(images, labels):\n",
    "    images = tf.cast(images, tf.float32) / 255.0\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply augmentations and prefetch the data to the GPU to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(augmenter_train, num_parallel_calls=tf.data.AUTOTUNE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(augmenter_val, num_parallel_calls=tf.data.AUTOTUNE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = test_ds.map(augmenter_val, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the augmented images\n",
    "image_batch = next(iter(train_ds.take(1)))[0]\n",
    "\n",
    "keras_cv.visualization.plot_image_gallery(\n",
    "    image_batch,\n",
    "    rows=3,\n",
    "    cols=3,\n",
    "    value_range=(0, 1),\n",
    "    show=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates: https://arxiv.org/abs/1708.07120 <br>\n",
    "This code is based on the implementation from, https://github.com/ageron/handson-ml3/blob/main/11_training_deep_neural_networks.ipynb <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# To find the optimal learning rate, we use an exponential learning rate schedule\n",
    "# We can plot the learning rate against the loss to find the optimal learning rate\n",
    "\n",
    "class ExponentialLearningRate(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.sum_of_epoch_losses = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        mean_epoch_loss = logs[\"loss\"]\n",
    "        new_sum_of_epoch_losses = mean_epoch_loss * (batch + 1)\n",
    "        batch_loss = new_sum_of_epoch_losses - self.sum_of_epoch_losses\n",
    "        self.sum_of_epoch_losses = new_sum_of_epoch_losses\n",
    "        self.rates.append(self.model.optimizer.learning_rate.numpy())\n",
    "        self.losses.append(batch_loss)\n",
    "        new_lr = self.model.optimizer.learning_rate * self.factor\n",
    "        self.model.optimizer.learning_rate.assign(new_lr)\n",
    "        \n",
    "def find_learning_rate(model, dataset, epochs=1, min_rate=1e-4, max_rate=1):\n",
    "    init_weights = model.get_weights()\n",
    "    num_samples = tf.data.experimental.cardinality(dataset).numpy()\n",
    "    iterations = math.ceil(num_samples / epochs)\n",
    "    factor = (max_rate / min_rate) ** (1 / iterations)\n",
    "    init_lr = model.optimizer.learning_rate.numpy()\n",
    "    model.optimizer.learning_rate.assign(min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(dataset, epochs=epochs, callbacks=[exp_lr])\n",
    "    model.optimizer.learning_rate.assign(init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses, \"b\")\n",
    "    plt.gca().set_xscale('log')\n",
    "    max_loss = losses[0] + min(losses)\n",
    "    plt.hlines(min(losses), min(rates), max(rates), color=\"k\")\n",
    "    plt.axis([min(rates), max(rates), 0, max_loss])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-cycle learning rate scheduler is a learning rate schedule that consists of three phases:\n",
    "1. The learning rate increases linearly from the initial learning rate to the maximum learning rate\n",
    "2. The learning rate decreases linearly from the maximum learning rate to the minimum learning rate\n",
    "3. The learning rate decreases linearly from the minimum learning rate to the final learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_lr=1e-3, start_lr=None,\n",
    "                 last_iterations=None, last_lr=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_lr = max_lr\n",
    "        self.start_lr = start_lr or max_lr / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_lr = last_lr or self.start_lr / 1000\n",
    "        self.iteration = 0\n",
    "\n",
    "    def _interpolate(self, iter1, iter2, lr1, lr2):\n",
    "        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
    "\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            lr = self._interpolate(0, self.half_iteration, self.start_lr,\n",
    "                                   self.max_lr)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            lr = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                   self.max_lr, self.start_lr)\n",
    "        else:\n",
    "            lr = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                   self.start_lr, self.last_lr)\n",
    "        self.iteration += 1\n",
    "        self.model.optimizer.learning_rate.assign(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-18\n",
    "\n",
    "We use the Residual blocks as defined in https://github.com/ageron/handson-ml3/blob/main/14_deep_computer_vision_with_cnns.ipynb <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "# Define the strandard convolutional layer \n",
    "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,\n",
    "                        padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                        use_bias=False)\n",
    "\n",
    "\n",
    "@register_keras_serializable(package='Custom', name='ResidualUnit')\n",
    "class ResidualUnit(keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.strides = strides\n",
    "        self.filters = filters\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                keras.layers.BatchNormalization()\n",
    "            ]\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "                \"filters\": self.filters,\n",
    "                \"strides\": self.strides,\n",
    "                \"activation\": keras.activations.serialize(self.activation)\n",
    "            })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(224, 224, 3)),\n",
    "    DefaultConv2D(64, kernel_size=7, strides=2),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"),\n",
    "])\n",
    "prev_filters = 64\n",
    "\n",
    "# Each ResidualUnit consists of two convolutional layers with the same number of filters\n",
    "# This gives us the ResNet-18 architecture\n",
    "for filters in [64] * 2 + [128] * 2 + [256] * 2 + [512] * 2:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "\n",
    "model.add(keras.layers.GlobalAvgPool2D())\n",
    "model.add(keras.layers.Dense(11, activation=\"softmax\"))\n",
    "\n",
    "loss = keras.losses.CategoricalFocalCrossentropy(weights)\n",
    "\n",
    "model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum = 0.9, weight_decay=1e-4),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal learning rate\n",
    "rates, losses = find_learning_rate(model, train_ds, epochs=1)\n",
    "plot_lr_vs_loss(rates, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.CategoricalFocalCrossentropy(weights),\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum = 0.9, weight_decay=1e-4),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "EPOCHS = 35\n",
    "# Set up callbacks\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "import datetime\n",
    "\n",
    "log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "onecycle = OneCycleScheduler(math.ceil(len(train_ds)) * EPOCHS, max_lr=1e-2)\n",
    "\n",
    "\n",
    "# Callbacks definition\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=20, verbose=1),\n",
    "    ModelCheckpoint('../models/best_model_18.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
    "    onecycle\n",
    "]\n",
    "\n",
    "\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_learning_curves(history, start_epoch=1):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy curves.\n",
    "\n",
    "    Args:\n",
    "        history: A History object generated from training a model.\n",
    "        start_epoch: The epoch that the training started from.\n",
    "    \"\"\"   \n",
    "\n",
    "    # Convert the history.history dict to a pandas DataFrame\n",
    "    df = pd.DataFrame(history.history)\n",
    "\n",
    "    # Plot the curves from the specified epoch onwards\n",
    "    df = df.iloc[start_epoch-1:]\n",
    "\n",
    "    # Set the style of seaborn for better visualization\n",
    "    sns.set(rc={'axes.facecolor': '#f0f0fc'}, style='darkgrid')\n",
    "\n",
    "    # Plotting the learning curves\n",
    "    plt.figure(figsize=(15,6))\n",
    "\n",
    "    # Plotting the training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.lineplot(x=df.index, y=df['loss'], color='royalblue', label='Train Loss')\n",
    "    sns.lineplot(x=df.index, y=df['val_loss'], color='orangered', linestyle='--', label='Validation Loss')\n",
    "    plt.title('Loss Evolution')\n",
    "\n",
    "    # Plotting the training and validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.lineplot(x=df.index, y=df['accuracy'], color='royalblue', label='Train Accuracy')\n",
    "    sns.lineplot(x=df.index, y=df['val_accuracy'], color='orangered', linestyle='--', label='Validation Accuracy')\n",
    "    plt.title('Accuracy Evolution')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(history, start_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model_performance(model, test_ds, class_labels):\n",
    "    \"\"\"\n",
    "    Evaluate the model's performance on the validation set and print the classification report.\n",
    "\n",
    "    Args:\n",
    "        model: A trained Keras model.\n",
    "        test_ds: A tf.data.Dataset object containing the test set.\n",
    "        class_labels: A list of class labels.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Initialize a list to hold all labels\n",
    "    true_labels = []\n",
    "\n",
    "    # Iterate over the dataset\n",
    "    for _ , labels in test_ds:\n",
    "        true_labels.append(np.argmax(labels.numpy()))  # Extract labels and convert to NumPy arrays\n",
    " \n",
    "    #  To get the predicted labels, we predict using the model  \n",
    "    predictions = model.predict(test_ds, steps=len(test_ds))\n",
    "    \n",
    "    # Take the argmax to get the predicted class indices.\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(true_labels, predicted_labels, target_names=class_labels)\n",
    "    print(report)\n",
    "    print('\\n')\n",
    "    \n",
    "    # Define a custom colormap\n",
    "    colors = [\"white\", \"royalblue\"]\n",
    "    cmap_cm = LinearSegmentedColormap.from_list(\"cmap_cm\", colors)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Plotting confusion matrix using seaborn\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, cmap=cmap_cm, fmt='d', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "model = keras.models.load_model('../models/best_model_18.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(model, test_ds, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can additionaly look at top 2 accuracy, as we have 11 classes it can be useful to see if the model is predicting the correct class as the second best class. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_2_accuracy(y_true, y_pred):\n",
    "    return keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=2)\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.CategoricalFocalCrossentropy(weights),\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.001, momentum = 0.9, weight_decay=1e-4),\n",
    "    metrics=[\"accuracy\", top_2_accuracy],\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy, test_top_2_accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Top-2 Accuracy: {test_top_2_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize images that has been misclassified an the probability distribution of the top 5 predictions. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to hold true and predicted labels\n",
    "true_labels = []\n",
    "images_list = []\n",
    "\n",
    "# Collect true labels and images\n",
    "for images, labels in test_ds:\n",
    "    true_labels.extend(np.argmax(labels.numpy(), axis=1))  # Extract labels and convert to NumPy arrays\n",
    "    images_list.extend(images.numpy())  # Collect images as well\n",
    "\n",
    "# Predict using the model\n",
    "predictions = model.predict(test_ds, steps=len(test_ds))\n",
    "\n",
    "# Get the predicted class indices\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Identify wrongly classified images\n",
    "wrong_indices = [i for i in range(len(true_labels)) if true_labels[i] != predicted_labels[i]]\n",
    "\n",
    "# Visualize wrongly classified images with their predicted probabilities\n",
    "def plot_wrong_classifications(images, true_labels, predicted_labels, predictions, wrong_indices, num_images=10):\n",
    "    plt.figure(figsize=(20, 40))\n",
    "    \n",
    "    for i, idx in enumerate(wrong_indices[:num_images]):\n",
    "        # Plot the image\n",
    "        ax_image = plt.subplot(num_images, 2, 2 * i + 1)\n",
    "        plt.imshow(images[idx])\n",
    "        plt.title(f\"True: {true_labels[idx]}, Pred: {predicted_labels[idx]}\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Plot the bar chart of probabilities\n",
    "        ax_bar = plt.subplot(num_images, 2, 2 * i + 2)\n",
    "        top_5_indices = np.argsort(predictions[idx])[-5:][::-1]\n",
    "        top_5_probs = predictions[idx][top_5_indices]\n",
    "        top_5_labels = top_5_indices\n",
    "        \n",
    "        ax_bar.barh(range(5), top_5_probs, color='blue')\n",
    "        ax_bar.set_yticks(range(5))\n",
    "        ax_bar.set_yticklabels(top_5_labels)\n",
    "        ax_bar.invert_yaxis()  # Invert y-axis to have the highest probability on top\n",
    "        ax_bar.set_xlabel('Probability')\n",
    "        ax_bar.set_title('Top 5 Predicted Probabilities')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot\n",
    "plot_wrong_classifications(images_list, true_labels, predicted_labels, predictions, wrong_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tumor-segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
